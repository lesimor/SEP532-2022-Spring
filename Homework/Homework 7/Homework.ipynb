{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP532 인공지능 이론과 실제\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "**Please write your student ID and name here!**\n",
    "- Student ID:\n",
    "- Name:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Action Recognition\n",
    "\n",
    "In this assignment, you are going to recognize an action of a given video using both a convolutional neural network and a recurrent neural network. Please follow the steps below to continue this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "Copy this example to your Google Colab and edit it to complete your assignment. We should be able to reproduce your results using your code and pre-trained model. Please double-check if your code runs without error properly. Submissions failed to run or reproduce the results will get a substantial penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# DO NOT EDIT THE FOLLOWING LINES\n",
    "# THESE LINES ARE FOR REPRODUCIBILITY\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the UCF101 dataset\n",
    "\n",
    "In this assignment, you will use the UCF101 which is an action recognition dataset of realistic action videos, collected from YouTube, having 101 action categories. (*Soomro, K., Zamir, A. R., & Shah, M. (2012). UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402.*)\n",
    "\n",
    "![UCF101 Dataset](images/ucf101.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCF101 dataset consists of 13,320 videos and their labels. Since your computing resource in Google Colab is somewhat limited, TA sampled half of the dataset, limited the length of videos to 64 frames, separated videos into frames and stored them to a single file in advance. \n",
    "\n",
    "Let's download and load this file. **(If the link below is not working, please let TA know to fix the link)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if not os.path.exists('ucf101.pickle'):\n",
    "    !wget -O 'ucf101.pickle' 'https://www.dropbox.com/s/2558ailo46px55j/ucf101.pickle?dl=1'\n",
    "\n",
    "    # If the link above is not working, you can also use the following link but it would be slower than the above.\n",
    "    # !wget -O 'ucf101.pickle' 'https://storage.keai.io/s/0PdikjRn2xFl2ks/download'\n",
    "    \n",
    "with open('ucf101.pickle', 'rb') as input_file:\n",
    "    dataset = pickle.load(input_file)\n",
    "    \n",
    "num_trains = len(dataset['train'])\n",
    "num_validations = len(dataset['validation'])\n",
    "num_tests = len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = [\n",
    "    'ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', \n",
    "    'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', \n",
    "    'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', \n",
    "    'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', \n",
    "    'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', \n",
    "    'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', \n",
    "    'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', \n",
    "    'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', \n",
    "    'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', \n",
    "    'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', \n",
    "    'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', \n",
    "    'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', \n",
    "    'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', \n",
    "    'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', \n",
    "    'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', \n",
    "    'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', \n",
    "    'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', \n",
    "    'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', \n",
    "    'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', \n",
    "    'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', \n",
    "    'YoYo', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what some of these videos and their corresponding labels look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def plot_frames(frames):\n",
    "    figure = plt.figure(figsize=(frames.shape[1] / 72, frames.shape[2] / 72), dpi=72)\n",
    "    image = plt.figimage(frames[0])\n",
    "    \n",
    "    def animate(step):\n",
    "        image.set_array(frames[step])\n",
    "        return (image, )\n",
    "    \n",
    "    video = FuncAnimation(\n",
    "        figure, animate, \n",
    "        frames=len(frames), interval=33, \n",
    "        repeat_delay=1, repeat=True\n",
    "    ).to_html5_video()\n",
    "    \n",
    "    display(HTML(video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frames, label in random.sample(dataset['train'], 3):\n",
    "    plot_frames(tf.stack([tf.image.decode_jpeg(frame) for frame in frames]))\n",
    "    print('Label:', index_to_label[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess the dataset\n",
    "\n",
    "Unlike images and text, video data contains both spatial and temporal information. Therefore, to handle these data, you will use both convolutional neural networks and recurrent neural networks to recognize an action of the videos.\n",
    "\n",
    "First, let's extract meaningful features from video frames using the pre-trained convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "Define a model to extract meaningful features from the given video frame using the pre-trained convolutional neural networks of your choice.\n",
    "\n",
    "This model should output a 1D vector for one given frame. That is,\n",
    "- In: `(1, 256, 256, 3)` → Out: `(1, dimension of features of your choice)`\n",
    "- In: `(5, 256, 256, 3)` → Out: `(5, dimension of features of your choice)`\n",
    "- In: `(number of frames, 256, 256, 3)` → Out: `(number of frames, dimension of features of your choice)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a model to extract features from the given video frame\n",
    "#       using the pre-trained convolutional neural networks of your choice.\n",
    "\n",
    "### START CODE HERE ###\n",
    "cnn_model = \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "To extract features from the given frames, TA provided `extract_features()` function. In this function, you need to **preprocess each frame** so it can be fed into the `cnn_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts features from the given frames using the defined cnn_model\n",
    "# - In: frames.shape = (number of frames, 256, 256, 3)\n",
    "# - Out: features.shape = (number of frames, dimension of features of your choice)\n",
    "def extract_features(frames, batch=32):\n",
    "    # TODO: Preprocess each frame so it can be fed into the `cnn_model`\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    features = tf.concat([\n",
    "        cnn_model(frames[index:index + batch])\n",
    "        for index in range(0, frames.shape[0], batch)\n",
    "    ], axis=0)\n",
    "    \n",
    "    if features.shape[0] < max_length:\n",
    "        features = tf.concat([\n",
    "            features,\n",
    "            tf.zeros((max_length - features.shape[0], *features.shape[1:]))\n",
    "        ], axis=0)\n",
    "    \n",
    "    return features.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can extract features from the video but this task is very time-consuming. Therefore, TA provided `preprocess_dataset()` function which takes a dataset, extracts features from the dataset, stores the features into a file, and loads the features from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "max_length = 32 # DO NOT CHANGE THIS NUMBER\n",
    "\n",
    "def decode_frames(frames):\n",
    "    return tf.stack([tf.image.decode_jpeg(frame) for frame in frames])\n",
    "\n",
    "def preprocess_dataset(dataset, filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as input_file:\n",
    "            return pickle.load(input_file)\n",
    "    else:\n",
    "        tensors = [\n",
    "            (extract_features(decode_frames(frames[:max_length])), label)\n",
    "            for frames, label in tqdm(dataset)\n",
    "        ]\n",
    "        \n",
    "        X, y = zip(*tensors)\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        with open(filename, 'wb') as output_file:\n",
    "            pickle.dump((X, y), output_file, protocol=4) # protocol=4 supports a byte objects larget than 4GB\n",
    "        \n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract features from all the datasets. **Please note that the first run takes up to 30 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = preprocess_dataset(dataset['train'], f'train-dataset-{max_length}-{num_trains}.pickle')\n",
    "validation_features, validation_labels = preprocess_dataset(dataset['validation'], f'validation-dataset-{max_length}-{num_validations}.pickle')\n",
    "test_features, test_labels = preprocess_dataset(dataset['test'], f'test-dataset-{max_length}-{num_tests}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, combine the features into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "batch_train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(512).batch(batch_size)\n",
    "batch_validation_dataset = tf.data.Dataset.from_tensor_slices((validation_features, validation_labels)).shuffle(512).batch(batch_size)\n",
    "batch_test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).shuffle(512).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the model\n",
    "All videos are transformed into 2D tensors via convolutional neural networks. To process these tensors, let's build a recurrent neural network.\n",
    "\n",
    "#### Problem 3\n",
    "Define a recurrent neural network to recognize one of `num_classes` actions from the given video. Becaue all videos have different lengths, your `lstm_model` should take account this into account. To do that, TA added `tf.keras.layers.Masking` layer in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(index_to_label)\n",
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.0), # DO NOT REMOVE THIS LAYER\n",
    "\n",
    "    # TODO: Define a recurrent neural network to recognize one of `num_classes` actions from the given video\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model\n",
    "In this assignment, you are going to manually train your model using `tf.GradientTape`.\n",
    "\n",
    "#### Problem 4\n",
    "Define an optimizer of your choice and optimize your `lstm_model` to minimize `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define an optimizer of your choice\n",
    "### START CODE HERE ###\n",
    "optimizer = \n",
    "### END CODE HERE ###\n",
    "\n",
    "def forward(features, actual):\n",
    "    predicted = lstm_model(features)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(actual, predicted)\n",
    "\n",
    "    return predicted, tf.reduce_mean(loss)\n",
    "\n",
    "def train_step(features, actual):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted, loss = forward(features, actual)\n",
    "    \n",
    "    # TODO: Optimize your `lstm_model` to minimize `loss`\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predicted, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch_dataset):\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    test_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for batch_features, batch_actual in batch_dataset:\n",
    "        batch_predicted, batch_loss = forward(batch_features, batch_actual)\n",
    "        \n",
    "        test_accuracy(batch_actual, batch_predicted)\n",
    "        test_loss(batch_loss)\n",
    "        \n",
    "    return test_accuracy.result(), test_loss.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5\n",
    "Train your `lstm_model` at least 10 epochs. *Please note that if you want to use your custom training loop, it is completely okay to use your loop instead of the code cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 # YOU CAN ADJUST THE NUMBER OF EPOCHS IF YOU WANT\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "with tqdm(total=epochs) as epoch_progress:\n",
    "    for epoch in range(epochs):\n",
    "        epoch_progress.set_description(f'Epoch {epoch + 1}')\n",
    "        \n",
    "        train_accuracy.reset_states()\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "        with tqdm(total=num_trains // batch_size) as batch_progress:\n",
    "            for batch, (batch_features, batch_actual) in enumerate(batch_train_dataset):\n",
    "                batch_progress.set_description(f'Batch {batch + 1}')\n",
    "                \n",
    "                batch_predicted, batch_loss = train_step(batch_features, batch_actual)\n",
    "                train_accuracy(batch_actual, batch_predicted)\n",
    "                train_loss(batch_loss)\n",
    "                \n",
    "                if (batch % 10) == 0:\n",
    "                    batch_progress.set_postfix(batch_loss=batch_loss.numpy())\n",
    "                batch_progress.update()\n",
    "        \n",
    "            validation_accuracy, validation_loss = evaluate(batch_validation_dataset)\n",
    "            batch_progress.set_postfix(\n",
    "                batch_loss=batch_loss.numpy(),\n",
    "                validation_accuracy=validation_accuracy.numpy(), \n",
    "                validation_loss=validation_loss.numpy())\n",
    "        \n",
    "        epoch_progress.set_postfix(\n",
    "            train_accuracy=train_accuracy.result().numpy(), \n",
    "            train_loss=train_loss.result().numpy(),\n",
    "            validation_accuracy=validation_accuracy.numpy(), \n",
    "            validation_loss=validation_loss.numpy())\n",
    "        epoch_progress.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate accuracy\n",
    "\n",
    "Let's evaluate the trained model using test dataset and print the test accuracy of the model. For your information, the accuracy of the model proposed by the authors who published the UCF101 dataset is 43.90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, test_loss = evaluate(batch_test_dataset)\n",
    "\n",
    "print(f'Test accuracy: {test_accuracy.numpy():.4f}')\n",
    "print(f'Test loss: {test_loss.numpy():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the below cell, you can try to recognize an action of the test videos using your trined `lstm_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frames, label in random.sample(dataset['test'], 3):\n",
    "    print('Acutal:', index_to_label[label])\n",
    "    \n",
    "    print('Predicted:')\n",
    "    features = extract_features(decode_frames(frames[:max_length]))\n",
    "    predicted = lstm_model(tf.expand_dims(features, 0))[0]\n",
    "    for confidence, index in zip(*tf.math.top_k(predicted, k=3)):\n",
    "        print(f'- {index_to_label[index]} ({confidence.numpy():.4f})')\n",
    "    \n",
    "    plot_frames(decode_frames(frames))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra points\n",
    "TAs will rank the submissions based on the test accuracy and assign extra points according to the rank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
